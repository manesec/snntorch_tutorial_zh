{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzIQBw28NL8h"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"400\">](https://github.com/jeshraghian/snntorch/)\n",
        "\n",
        "# snnTorch - 2nd Order Spiking Neuron Models\n",
        "## Tutorial 4\n",
        "### By Jason K. Eshraghian (www.ncg.ucsc.edu)\n",
        "### 偏白话文翻译： manesec\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_4_advanced_neurons.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H56eQUdm3IuT"
      },
      "source": [
        "snnTorch 教程系列基于以下论文。如果您发现这些资源或代码对您有帮助，请考虑引用以下来源：\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". arXiv preprint arXiv:2109.12894, September 2021.](https://arxiv.org/abs/2109.12894) </cite>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_Qv7kzNOz6"
      },
      "source": [
        "# 介绍\n",
        "\n",
        "在本教程中，您将：\n",
        "* 了解改进版的 带泄漏整合发放模型 (LIF) 神经元模型： 突触（`Synaptic`） 和 `Alpha`\n",
        "\n",
        " 单击以下单元格并按 “Shift+Enter”，安装 snnTorch 的最新 PyPi 发行版。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPQITvDuNNJg"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31NkMZnxBFZ4"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2kBGXe5K_xWh"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Settings\n",
        "\n",
        "def plot_spk_cur_mem_spk(spk_in, syn_rec, mem_rec, spk_rec, title):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(4, figsize=(8,7), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [0.4, 1, 1, 0.4]})\n",
        "\n",
        "  # Plot input current\n",
        "  splt.raster(spk_in, ax[0], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[0].set_ylabel(\"Input Spikes\")\n",
        "  ax[0].set_title(\"Synaptic Conductance-based Neuron Model With Input Spikes\")\n",
        "  ax[0].set_yticks([])\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(syn_rec.detach().numpy())\n",
        "  ax[1].set_ylim([0, 0.5])\n",
        "  ax[1].set_ylabel(\"Synaptic Current ($I_{syn}$)\")\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[2].plot(mem_rec.detach().numpy())\n",
        "  ax[2].set_ylim([0, 1.5])\n",
        "  ax[2].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "  ax[2].axhline(y=1, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  # Plot output spike using spikeplot\n",
        "  splt.raster(spk_rec, ax[3], s=400, c=\"black\", marker=\"|\")\n",
        "  plt.ylabel(\"Output spikes\")\n",
        "  ax[3].set_yticks([])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_spk_mem_spk(spk_in, mem_rec, spk_rec, title):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(3, figsize=(8,6), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [0.4, 1, 0.4]})\n",
        "\n",
        "  # Plot input current\n",
        "  splt.raster(spk_in, ax[0], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[0].set_ylabel(\"Input Spikes\")\n",
        "  ax[0].set_title(title)\n",
        "  ax[0].set_yticks([])\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem_rec.detach())\n",
        "  ax[1].set_ylim([0, 0.6])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "  ax[1].axhline(y=0.5, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  # Plot output spike using spikeplot\n",
        "  splt.raster(spk_rec, ax[2], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[2].set_yticks([])\n",
        "  ax[2].set_ylabel(\"Output Spikes\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw0Zfve-vHGb"
      },
      "source": [
        "# 1. 基于突触(Synaptic) 电导的 LIF 神经元模型\n",
        "之前的教程中探讨的神经元模型，假设输入电压尖峰会导致突触电流的瞬间跃变，然后影响膜电位。实际上，尖峰会经过神经递质从突触前神经元逐渐释放到突触后神经元。基于突触电导的 LIF 模型考虑了输入电流的渐进时间动态。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvbklKvTd3qY"
      },
      "source": [
        "## 1.1 突触电流建模\n",
        "\n",
        "如果突触前神经元 (pre-synaptic neruon) 放电，电压尖峰会沿着神经元的轴突传递。然后下图的囊泡(vesicles) 将会释放 神经传递物质(neurotransmitters) 到 突触(synaptic) 间隙。它们激活突触后受体，直接影响流入突触后神经元的有效电流。下图显示了两种类型的兴奋性受体：AMPA 和 NMDA。\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_6_synaptic.png?raw=true' width=\"600\">\n",
        "</center>\n",
        "\n",
        "最简单的突触电流模型假设电流在非常快的时间尺度上增加，然后是相对较慢的指数衰减，如上面的 AMPA 图标所示。 这与拉皮克（Lapicque）模型的膜势动力学非常相似。\n",
        "\n",
        "突触模型有两个指数衰减项：$I_{\\rm syn}(t)$ 和 $U_{\\rm mem}(t)$。 $I_{\\rm syn}(t)$ 的后续项之间的比率（即衰减率）设置为 $\\alpha$，$U(t)$ 的后续项之间的比率设置为 $\\beta$：\n",
        "\n",
        "$$ \\alpha = e^{-\\Delta t/\\tau_{\\rm syn}}$$\n",
        "\n",
        "$$ \\beta = e^{-\\Delta t/\\tau_{\\rm mem}}$$\n",
        "\n",
        "其中单个时间步长的持续时间为 $\\Delta t = 1$。 $\\tau_{\\rm syn}$ 对突触电流的时间常数进行建模，其方式与 $\\tau_{\\rm mem}$ 对膜电位的时间常数进行建模类似。 $\\beta$ 的导出方式与之前的教程完全相同，与 $\\alpha$ 的方法类似：\n",
        "\n",
        "$$I_{\\rm syn}[t+1]=\\underbrace{\\alpha I_{\\rm syn}[t]}_\\text{decay} + \\underbrace{WX[t+1]}_\\text{input}$$\n",
        "\n",
        "$$U[t+1] = \\underbrace{\\beta U[t]}_\\text{decay} + \\underbrace{I_{\\rm syn}[t+1]}_\\text{input} - \\underbrace{R[t]}_\\text{reset}$$\n",
        "\n",
        "与之前的 LIF 神经元相同的尖峰条件仍然成立：\n",
        "\n",
        "$$S_{\\rm out}[t] = \\begin{cases} 1, &\\text{if}~U[t] > U_{\\rm thr} \\\\\n",
        "0, &\\text{otherwise}\\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbbNrPL_guwN"
      },
      "source": [
        "## 1.2 snnTorch 中的突触神经元模型\n",
        "\n",
        "基于突触电导的神经元模型将突触电流动力学与被动膜结合起来，它必须通过两个输入参数实例化：\n",
        "* $\\alpha$：突触电流的衰减率\n",
        "* $\\beta$：膜电位衰减率（与 Lapicque 相同）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJySGu7R-ozz"
      },
      "outputs": [],
      "source": [
        "# Temporal dynamics\n",
        "alpha = 0.9\n",
        "beta = 0.8\n",
        "num_steps = 200\n",
        "\n",
        "# Initialize 2nd-order LIF neuron\n",
        "lif1 = snn.Synaptic(alpha=alpha, beta=beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJNO461Rgcm2"
      },
      "source": [
        "使用此神经元与之前的 LIF 神经元完全相同，但现在添加了突触电流`syn`作为输入和输出：\n",
        "\n",
        "**输入**\n",
        "* `spk_in`：每个加权输入电压尖峰 $WX[t]$ 依次传入\n",
        "* `syn`: 上一个时间步的突触电流 $I_{\\rm syn}[t-1]$\n",
        "* `mem`：上一个时间步的膜电位 $U[t-1]$\n",
        "\n",
        "**输出**\n",
        "* `spk_out`：输出尖峰$S[t]$（如果有尖峰则为“1”；如果没有尖峰则为“0”）\n",
        "* `syn`: 当前时间步的突触电流 $I_{\\rm syn}[t]$\n",
        "* `mem`：当前时间步的膜电位 $U[t]$\n",
        "\n",
        "这些都需要是`torch.Tensor`类型。 请注意，神经元模型的时间后移了一步，但不失一般性。\n",
        "\n",
        "应用周期性尖峰输入来查看电流和膜如何随时间变化："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zv_ql1Rg2T4"
      },
      "outputs": [],
      "source": [
        "# Periodic spiking input, spk_in = 0.2 V\n",
        "w = 0.2\n",
        "spk_period = torch.cat((torch.ones(1)*w, torch.zeros(9)), 0)\n",
        "spk_in = spk_period.repeat(20)\n",
        "\n",
        "# Initialize hidden states and output\n",
        "syn, mem = lif1.init_synaptic()\n",
        "spk_out = torch.zeros(1)\n",
        "syn_rec = []\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# Simulate neurons\n",
        "for step in range(num_steps):\n",
        "  spk_out, syn, mem = lif1(spk_in[step], syn, mem)\n",
        "  spk_rec.append(spk_out)\n",
        "  syn_rec.append(syn)\n",
        "  mem_rec.append(mem)\n",
        "\n",
        "# convert lists to tensors\n",
        "spk_rec = torch.stack(spk_rec)\n",
        "syn_rec = torch.stack(syn_rec)\n",
        "mem_rec = torch.stack(mem_rec)\n",
        "\n",
        "plot_spk_cur_mem_spk(spk_in, syn_rec, mem_rec, spk_rec,\n",
        "                     \"Synaptic Conductance-based Neuron Model With Input Spikes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB3L2bhxhhCs"
      },
      "source": [
        "该模型还具有 \"复位机制\"（`reset_mechanism`）和 \"阈值\"（`threshold`）这两个可选的输入参数，正如拉皮克神经元模型所描述的那样。总之，每个尖峰都会对突触电流 $I_{\\rm syn}$ 产生移位指数衰减，并将所有电流相加。然后，该电流通过[教程 2](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)中推导的被动膜方程进行积分，从而产生输出尖峰。下面是这一过程的示意图。\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_7_stein.png?raw=true' width=\"600\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvpFjeq4kkZ9"
      },
      "source": [
        "## 1.3 一阶神经元与二阶神经元\n",
        "\n",
        "一个自然而然产生的问题是：什么时候我该使用一阶 LIF 神经元？什么时候我该使用二阶 LIF 神经元？\n",
        "\n",
        "**使用二阶 LIF 神经元作为最优解时**\n",
        "\n",
        "* 如果输入数据的时间单位发生在较长的区间\n",
        "* 或者如果输入尖峰模式是稀疏的\n",
        "\n",
        "通过两个具有两个衰减项（$\\alpha$ 和 $\\beta$）的递归方程，该神经元模型能够 \"维持\" 较长时间的输入尖峰。通过两个具有两个衰减项（$\\alpha$ 和 $\\beta$）的递归方程，这个神经元模型能够在更长的时间内 \"维持\" 输入尖峰。这有利于保持长期关系。\n",
        "\n",
        "另一种用例也可能是：\n",
        "\n",
        "* 当时间代码很重要时\n",
        "\n",
        "如果您关心尖峰的精确时间，那么对于二阶神经元来说控制它似乎更容易。 在`Leaky`模型中，将与输入直接同步地触发尖峰。 对于二阶模型，膜电位被“平滑”（即突触电流模型对膜电位进行低通滤波），这意味着 $U[t]$ 经历有限的上升时间。 这在前面的模拟中很明显，输出尖峰相对于输入尖峰有延迟。\n",
        "\n",
        "**使用一阶ELF 神经元作为最优解时**\n",
        "* 任何不属于上述情况的情况，有时也属于上述情况。\n",
        "\n",
        "在一阶神经元模型（如 `Leaky`）中少了一个方程，反向传播过程就会变得简单一些。尽管如此，当 $\\alpha=0$ 时，`突触（Synaptic）` 模型在功能上等同于 `Leaky` 模型。在我自己对简单数据集的超参数扫描中，最佳结果似乎是将 $\\alpha$ 尽可能推近 0。随着数据复杂度的增加，$\\alpha$ 可能会越来越大。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_vgXmMkt-C"
      },
      "source": [
        "# 2. Alpha 模型（魔改的尖峰反应模型）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G30m1mX3kwKK"
      },
      "source": [
        "尖峰响应模型（SRM）或 \"阿尔法\" 神经元的递归版本也可用 `snn.Alpha` 调用。迄今为止，所有神经元模型都是基于被动膜模型，使用常微分方程来描述其过程。\n",
        "\n",
        "而 SRM 系列模型则用滤波器来解释。当输入尖峰到达时，该尖峰与滤波器卷积，从而得到膜电位响应。滤波器的形式可以是指数形式，如 Lapicque 神经元，也可以是更复杂的形式，如指数之和。SRM 模型很有吸引力，因为它们可以任意添加折射性、阈值适应性和其他任何特征，只需将它们嵌入滤波器即可。\n",
        "\n",
        "\n",
        "<left>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/exp.gif?raw=true' width=\"400\">\n",
        "</left>\n",
        "\n",
        "<right>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/alpha.gif?raw=true' width=\"400\">\n",
        "</right>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC8fHWlipeqW"
      },
      "source": [
        "\n",
        "## 建立 Alpha 神经元模型\n",
        "从形式上看，这一过程可以用以下方式表示\n",
        "\n",
        "$$U_{\\rm mem}(t) = \\sum_i W(\\epsilon * S_{\\rm in})(t)$$\n",
        "\n",
        "其中，输入的尖峰信号 $S_{\\rm in}$ 与尖峰响应核 $\\epsilon( \\cdot )$ 进行卷积。尖峰响应按突触权重 $W$ 缩放。在上图中，左边的核是一个指数衰减函数，相当于拉皮克的一阶神经元模型。右图中，核是一个阿尔法函数：\n",
        "\n",
        "$$\\epsilon(t) = \\frac{t}{\\tau}e^{1-t/\\tau}\\Theta(t)$$\n",
        "\n",
        "其中，$\\tau$ 是阿尔法核的时间常数，$\\Theta$ 是海维塞德阶跃函数。大多数基于核的方法都采用阿尔法函数，因为它提供了一个时间延迟，对于需要指定神经元精确尖峰时间的时序编码非常有用。\n",
        "\n",
        "在 snnTorch 中，尖峰响应模型并不是直接作为滤波器来实现的。相反，它被重铸为递归形式，这样只需要使用上一时间步计算的值，用来计算下一组值。这大大减少了学习过程中的内存开销。\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_9_alpha.png?raw=true' width=\"600\">\n",
        "</center>\n",
        "\n",
        "由于膜电位现在是由两个指数之和决定的，因此每个指数都有自己独立的衰减率。$\\alpha$ 定义了正指数的衰减率，而 $\\beta$ 定义了负指数的衰减率。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tL8gLiAl-gW"
      },
      "outputs": [],
      "source": [
        "alpha = 0.8\n",
        "beta = 0.7\n",
        "\n",
        "# initialize neuron\n",
        "lif2 = snn.Alpha(alpha=alpha, beta=beta, threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmfuWFoMl8YG"
      },
      "source": [
        "使用该神经元与之前的神经元相同，但是两个指数函数的总和需要将突触电流`syn`拆分为 `syn_exc`和`syn_inh`分量：\n",
        "\n",
        "**输入**\n",
        "* `spk_in`：每个加权输入电压尖峰 $WX[t]$ 依次传入\n",
        "* `syn_exc`: 上一个时间步的兴奋性突触后电流 $I_{\\rm syn-exc}[t-1]$\n",
        "* `syn_inh`: 上一个时间步的抑制性突触后电流 $I_{\\rm syn-inh}[t-1]$\n",
        "* `mem`: 膜电位 $U_{\\rm mem}[t-1]$ 当前时间 $t$ 在上一个时间步\n",
        "\n",
        "**输出**\n",
        "* `spk_out`：当前时间步的输出尖峰 $S_{\\rm out}[t]$ （如果有尖峰则为“1”；如果没有尖峰则为“0”）\n",
        "* `syn_exc`：当前时间步 $t$ 的兴奋性突触后 $I_{\\rm syn-exc}[t]$\n",
        "* `syn_inh`：当前时间步 $t$ 的抑制性突触后电流 $I_{\\rm syn-inh}[t]$\n",
        "* `mem`：当前时间步的膜电位 $U_{\\rm mem}[t]$\n",
        "\n",
        "与所有其他神经元模型一样，这些模型必须是`torch.Tensor`类型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQrT4VV5mSVv"
      },
      "outputs": [],
      "source": [
        "# input spike: initial spike, and then period spiking\n",
        "w = 0.85\n",
        "spk_in = (torch.cat((torch.zeros(10), torch.ones(1), torch.zeros(89),\n",
        "                     (torch.cat((torch.ones(1), torch.zeros(9)),0).repeat(10))), 0) * w).unsqueeze(1)\n",
        "\n",
        "# initialize parameters\n",
        "syn_exc, syn_inh, mem = lif2.init_alpha()\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# run simulation\n",
        "for step in range(num_steps):\n",
        "  spk_out, syn_exc, syn_inh, mem = lif2(spk_in[step], syn_exc, syn_inh, mem)\n",
        "  mem_rec.append(mem.squeeze(0))\n",
        "  spk_rec.append(spk_out.squeeze(0))\n",
        "\n",
        "# convert lists to tensors\n",
        "mem_rec = torch.stack(mem_rec)\n",
        "spk_rec = torch.stack(spk_rec)\n",
        "\n",
        "plot_spk_mem_spk(spk_in, mem_rec, spk_rec, \"Alpha Neuron Model With Input Spikes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH9O-kPzooXw"
      },
      "source": [
        "与之前的 Lapicque 和 突触（Synaptic） 的模型一样，Alpha 型号也有修改阈值和复位机制的选项。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erpkjgKCV8jW"
      },
      "source": [
        "## 2.2 实际情况是\n",
        "\n",
        "正如前面提到的突触神经元，模型越复杂，训练过程中的反向传播过程就越复杂。在我自己的实验中，我还没有发现 Alpha 神经元模型 优于 突触(Synaptic)神经元模型 和 漏(Leaky)神经元模型 的情况。通过正负指数进行学习似乎只会增加梯度计算过程的难度，抵消在更复杂的神经元动态过程中的潜在优势。\n",
        "\n",
        "然而，当 SRM 模型以时变核的形式表示（而不是像这里这样以递归模型的形式表示）时，它的表现似乎与较简单的神经元模型一样好。例如，请参阅以下论文：\n",
        "\n",
        "> [*Sumit Bam Shrestha and Garrick Orchard, \"SLAYER: Spike layer error reassignment in time\", Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 1419-1328, 2018.*](https://arxiv.org/abs/1810.08646)\n",
        "\n",
        "加入 Alpha 神经元的目的是为将基于 SRM 的模型移植到 snnTorch 提供一个选项，尽管在这里对它们进行本机训练似乎不太有效。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGCY7g7R0Usy"
      },
      "source": [
        "# 结论\n",
        "\n",
        "我们已经介绍了 snnTorch 中的所有 LIF 神经元模型。简单总结一下：\n",
        "\n",
        "+ **Lapicque**：直接基于 RC 电路参数的物理精确模型\n",
        "+ **漏 (Leaky)**：简化的一阶模型\n",
        "+ **突触 (Synaptic)**：二阶模型，考虑突触电流演变\n",
        "+ **Alpha**：膜电位跟踪Alpha函数的二阶模型\n",
        "\n",
        "一般来说，\"Leaky\" 和 \"Synaptic\" 似乎对训练网络最有用。\"Lapicque\" 适合演示物理上精确的模型，而 \"Alpha\" 只用于捕捉 SRM 神经元的行为。\n",
        "\n",
        "使用这些略微高级的神经元构建网络的步骤与 [教程 3](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)完全相同。\n",
        "\n",
        "文档[可在此处](https://snntorch.readthedocs.io/en/latest/snntorch.html)供参考。\n",
        "\n",
        "如果您喜欢这个项目，请考虑在 GitHub 上的 repo ⭐ 星级，因为这是最简单、最好的支持方式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTRCyd0Xa-QK"
      },
      "source": [
        "## Further Reading\n",
        "* [Check out the snnTorch GitHub project here.](https://github.com/jeshraghian/snntorch)\n",
        "* [snnTorch documentation](https://snntorch.readthedocs.io/en/latest/snntorch.html) of the Lapicque, Leaky, Synaptic, and Alpha models\n",
        "* [*Neuronal Dynamics:\n",
        "From single neurons to networks and models of cognition*](https://neuronaldynamics.epfl.ch/index.html) by\n",
        "Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski.\n",
        "* [Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems](https://mitpress.mit.edu/books/theoretical-neuroscience) by Laurence F. Abbott and Peter Dayan"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Untitled17.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
